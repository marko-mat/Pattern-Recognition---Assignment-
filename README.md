The following is what the code itself is answering... Using a dataset which can be noted from within the code.

(a) (3 marks) Assume that the prior probabilities for the first two categories are equal P(ω1) =
P(ω2) = 1/2 and P(ω3) = 0 and design a dichotomizer for those two categories using only the x1
feature value.
(b) (3 marks) Determine the empirical training error on your samples, i.e., the percentage of points
misclassified.
(c) (2 marks) Use the Bhattacharyya bound to bound the error you will get on novel patterns drawn
from the distributions.
(d) (8 marks) Repeat all of the above, but now use two feature values, x1, and x2.
(e) (8 marks) Repeat, but use all three feature values.
(f) (1 marks) Discuss your results. In particular, is it ever possible for a finite set of data that the
empirical error might be larger for more data dimensions?
